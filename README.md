# The Developer's Guide to AI - From Prompts to Agents

This repository contains the code for the book **The Developer's Guide to AI - From Prompts to Agents**.  You pre-order the book or get early access here:

[The Developer's Guide to AI: From Prompts to Agent (EARLY ACCESS)](https://nostarch.com/developers-guide-to-AI)

If you're using the examples or reading the early access edition:

__Your name could be in the final version of the book!__

The book is still in early access and we want your help!

If you:

- Run into any issues with the examples
- Have recommendations to improve our documentation
- Notice any issues in the book

Please let us know!  Submit the issue here:

[developers-guide-to-ai:issues](https://github.com/jorshali/developers-guide-to-ai/issues)

If your issue results in a change to the book text, a change to the examples, or, even better, an accepted pull request, your name will be added to the Acknowledgements in the final version of the book!

# Table of Contents

- [About the Book](#about-the-book)
- [Pre-trained AI Models](#pre-trained-ai-models)
- [Prerequisites](#prerequisites)
- [About This README](#about-this-readme)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Parts Overview](#parts-overview)
- [Python Environments (Recommended)](#python-environments-recommended)

# About the Book

Most developers already know AI is powerful. What they don’t know is how to actually use it in their own codebase without becoming a data scientist. Too often, AI features stall out at brittle prototypes, half-baked search functions, or “smart” features that collapse the moment requirements change. That’s where this book comes in. We wrote this book with the everyday software developer in mind. The dev who works in a company and wants to grow their career by meeting the demands of the industry without losing time in the areas that might not be relevant to their current company or role.

Written by seasoned developers Jacob Orshalick, Jerry M. Reghunadh, and Danny Thompson, this book teaches you how to integrate and customize large language models (LLMs) and other pre-trained AI models to solve real-world problems. 

Instead of drowning you in theory, this book gives you:

- Intelligent automation: Automate repetitive work by calling LLMs directly from your own applications and streaming intelligent responses to the UI.
- Practical Paths: Build production-ready AI features with tools you already know and some you don't.
- Clarity through the hype: Learn where AI actually makes sense in your applications (and where it doesn’t).
- Fewer dead-ends: Avoid wasted cycles by understanding limitations, costs, and trade-offs before you build.
- Competitive edge: Discover how AI can help you improve search, personalization, automation, and more.

We focus on practical applications using pre-trained AI models (LLMs and more) through accessible APIs and SDKs, allowing you to build AI-powered applications in the languages you already know.

# Pre-trained AI Models

The concepts and practical approaches presented in the Developer's Guide to AI are generally AI model agnostic, but we will be calling Ollama models in many of the examples.  Ollama provides a local API to open models that will only cost you the power required to power your laptop.  While some examples show how you can connect to hosted models (e.g. OpenAI APIs), keep in mind that to use these models you will need to register for an API key and pay for usage.

# Prerequisites

- Basic knowledge of JavaScript
- [Node.js](https://nodejs.org/) (Version 24.8.0)
- [Python](https://www.python.org/) (Version 3.12)
- [Git](https://git-scm.com/)

Recommended:

- [Homebrew (Mac)](https://brew.sh)
- [Python Virtual Environment](#virtual-environment-setup)

# About This README

The initial version of this README file was generated by the `gemini-cli` tool. The `instruction.md` file in this repository contains the set of instructions that were used to generate this file. The `gemini-cli` tool used these instructions to understand the project structure and generate the comprehensive overview of the project that you see here.

# Project Structure

The project is organized into parts. Each part corresponds to a part with in book **The Developer's Guide to AI - From Prompts to Agents**.

- `part1/`: Getting started with AI
- `part2/`: Prompt Engineering
- `part3/`: Vector Databases and RAG (Retrieval-Augmented Generation)
- `part4/`: Fine-tuning
- `part5/`: Agents

# Getting Started

1. Clone this repository:
   ```bash
   git clone https://github.com/yourusername/developers-guide-to-ai.git
   cd developers-guide-to-ai
   ```

2. Install dependencies for a specific part (example for part1):
   ```bash
   cd part1/getting_started/section1
   npm install
   ```

3. Follow the instructions in each part's `README.md` for specific setup and execution steps.

# Parts Overview

## Part 1: Getting Started with AI

This part provides the foundation for working with AI, featuring both JavaScript and Python implementations.

| Project | Description |
| --- | --- |
| [part1/client/](part1/client/) | A React-based frontend application that consumes the LLM API. It demonstrates how to handle streaming responses from the server and manage UI states, providing a simple interface to ask questions and see the LLM's response in real-time. |
| [part1/getting_started/](part1/getting_started/) | Contains two Express.js server implementations. `section1` provides a basic REST service that interfaces with a local LLM using LangChain. `section2` enhances this by implementing real-time streaming of LLM responses, showing how to create a more responsive user experience. |
| [part1/getting_started_python/](part1/getting_started_python/) | A FastAPI-based Python alternative to the JavaScript server. It showcases Python's asynchronous capabilities for LLM interactions, including streaming support, and demonstrates how to integrate with the same LLM backend using Python's ecosystem. |

---

## Part 2: Prompt Engineering

This part dives deeper into LLM capabilities with practical examples of prompt engineering techniques.

| Project | Description |
| --- | --- |
| [part2/basic_examples/](part2/basic_examples/) | A collection of Python scripts demonstrating fundamental prompt engineering concepts. It includes examples of temperature settings, tokenization, prompt templates, and guard rails to control LLM behavior. |
| [part2/basic_examples_javascript/](part2/basic_examples_javascript/) | JavaScript examples covering chunking, creating embeddings, and tokenization. These scripts show how to perform common NLP tasks in a JavaScript environment. |
| [part2/first_call_chain/](part2/first_call_chain/) | A simple FastAPI server that demonstrates how to create a basic LLM chain using LangChain. It shows how to combine a prompt template with an LLM to answer questions. |
| [part2/structured_output/](part2/structured_output/) | Python scripts that illustrate how to get structured data (JSON) from an LLM. It includes examples of sentiment analysis and data extraction using both the Ollama SDK and the `instructor` library. |

---

## Part 3: Vector Databases and RAG (Retrieval-Augmented Generation)

This part focuses on building advanced AI applications using Retrieval-Augmented Generation (RAG) and vector databases.

| Project | Description |
| --- | --- |
| [part3/advanced_rag/](part3/advanced_rag/) | An advanced RAG (Retrieval-Augmented Generation) implementation that uses a combination of BM25 and vector search (ensemble retriever) along with multi-query retrieval to enhance search results. It also includes a reranker to improve the relevance of the retrieved documents. |
| [part3/basic_examples/](part3/basic_examples/) | A Python script for experimenting with different text chunking strategies using LangChain's `RecursiveCharacterTextSplitter`. |
| [part3/client/](part3/client/) | A React-based chat interface that interacts with a RAG API. It supports conversational history and displays streaming responses from the backend. |
| [part3/conversational_rag_example/](part3/conversational_rag_example/) | A FastAPI server that implements a conversational RAG (Retrieval-Augmented Generation) system. It uses a vector store to retrieve relevant documents and maintains conversation history to provide context-aware responses. |
| [part3/parent_document_retriever_rag/](part3/parent_document_retriever_rag/) | A RAG (Retrieval-Augmented Generation) implementation that uses the `ParentDocumentRetriever` to handle large documents. This approach splits documents into smaller chunks for embedding and retrieval, but returns the larger parent document for better context. |
| [part3/rag_examples/](part3/rag_examples/) | A set of Python scripts that demonstrate different ways to build a RAG-based chatbot. It includes examples using both LangChain and the Ollama SDK. |
| [part3/simple_rag_javascript/](part3/simple_rag_javascript/) | A Node.js server that implements a simple RAG system in JavaScript. It uses an in-memory vector store and the `ParentDocumentRetriever` to answer questions based on a set of articles. |
| [part3/vector_databases/](part3/vector_databases/) | A collection of Python scripts that showcase various use cases for vector databases, including document search, product recommendation, and interactive product search with filtering. |

---

## Part 4: Fine-tuning

This part provides a comprehensive guide to fine-tuning models for specific tasks. It covers the entire lifecycle, from data preparation to model evaluation.

| Project | Description |
| --- | --- |
| [part4/01-dataset.ipynb](part4/01-dataset.ipynb) | A Jupyter notebook that demonstrates how to load raw data, split it into training, validation, and test sets, and save it as a Hugging Face dataset. |
| [part4/02.zeroShot.ipynb](part4/02.zeroShot.ipynb) | This notebook shows how to perform zero-shot classification using both a DistilBERT model and a Qwen model. It evaluates the accuracy of each model on a test dataset. |
| [part4/03-finetune-classificationModel.ipynb](part4/03-finetune-classificationModel.ipynb) | A notebook that fine-tunes a DistilBERT model for sequence classification. It covers tokenization, training, and evaluation of the fine-tuned model. |
| [part4/04-chat-examples.ipynb](part4/04-chat-examples.ipynb) | This notebook provides examples of how to interact with a chat model (Qwen) using different prompting strategies. |
| [part4/05-finetune-dataset.ipynb](part4/05-finetune-dataset.ipynb) | A notebook that prepares a dataset for fine-tuning a large language model (LLM). It transforms the data into a chat format suitable for training. |
| [part4/06-finetune-llm.ipynb](part4/06-finetune-llm.ipynb) | This notebook demonstrates how to fine-tune a Qwen LLM using LoRA (Low-Rank Adaptation). It sets up the trainer and runs the fine-tuning process. |
| [part4/07-test-ft-llm.ipynb](part4/07-test-ft-llm.ipynb) | A notebook for evaluating the fine-tuned LLM. It loads the model and tests its performance on the test set. |
| [part4/data/](part4/data/) | Contains the datasets used for fine-tuning, including the raw data and the processed datasets in Hugging Face format. |
| [part4/rawData/](part4/rawData/) | Contains the raw CSV files used as the source for the datasets. |

---

## Part 5: Agents

This part explores the concept of AI agents, which can use tools to interact with their environment and solve complex tasks.

| Project | Description |
| --- | --- |
| [part5/01-helloAgents.py](part5/01-helloAgents.py) | A Python script that introduces the `smol-agents` library. It demonstrates how to create a simple agent with tools like web search and browsing to answer a question. |
| [part5/02.findingEarningsReport.py](part5/02.findingEarningsReport.py) | This script showcases a more advanced agent that acts as a financial analyst. It uses tools to find and compare the quarterly revenues of different companies. |
| [part5/mcp/](part5/mcp/) | A project that demonstrates the Multi-Agent Collaboration Protocol (MCP). It includes an Express.js server with some API endpoints and an MCP server that exposes these endpoints as tools for agents to use. It also contains an example of an expense tracker agent that interacts with a MongoDB database. |

# Python Environments \[Recommended\]

It's recommended that you setup a Python virtual environment for running the Python examples.  To setup the environment, you can use `pyenv` and `pyenv-virtualenv` to create a virtual environment for each project. After setting up the virtual environment, you can install the required packages using `pip`.

_Note:_  There are many tools for creating virtual environments in Python.  Feel free to choose your own preferred option.

## Install `pyenv` and `pyenv-virtualenv`

Follow the instructions found here to setup `pyenv` for your environment:

[Install pyenv](https://github.com/pyenv/pyenv?tab=readme-ov-file#installation)

[Install pyenv-virtualenv](https://github.com/pyenv/pyenv-virtualenv?tab=readme-ov-file#installation)

## Install Python version 3.12.11

The following command installs the necessary Python version using `pyenv`:

```
pyenv install 3.12.11
```

## Virtual Environment Setup

Setup your virtual environment with the installed Python version:

```
$ pyenv virtualenv 3.12.11 developers-guide-to-ai-part1
```

Each part of the book has a specific set of dependencies for its examples defined in a `requirements.txt` file.  When you get to a new part, you can create a specific environment by running the command above and simply changing the `part` number.

Now you need to activate your virtual environment:

```
$ pyenv activate developers-guide-to-ai-part1
```

Now that you've activate the virtual environment, you can navigate to the example directory and install the required packages using `pip`:

```
pip install -r requirements.txt
```