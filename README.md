# The Developer's Guide to AI - From Prompts to Agents

This repository contains the code for the book **The Developer's Guide to AI - From Prompts to Agents**.

The book is meant to be a comprehensive guide that makes AI approachable for developers. This book is designed to jumpstart your AI learning journey. No need for advanced Python, neural network training, or a PhD in statistics. All you need is basic JavaScript knowledge and a willingness to learn.

We focus on practical applications using pre-trained AI models (LLMs and more) through accessible APIs and SDKs, allowing you to build AI-powered applications in the languages you already know.

# Table of Contents

- [Prerequisites](#prerequisites)
- [About This README](#about-this-readme)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
- [Parts Overview](#parts-overview)

# Prerequisites

- Basic knowledge of JavaScript
- [Node.js](https://nodejs.org/)
- [Python](https://www.python.org/)
- [Git](https://git-scm.com/)

# About This README

This README file was generated by the `gemini-cli` tool. The `instruction.md` file in this repository contains the set of instructions that were used to generate this file. The `gemini-cli` tool used these instructions to understand the project structure and generate the comprehensive overview of the project that you see here.

# Project Structure

The project is organized into parts. Each part corresponds to a part with in book **The Developer's Guide to AI - From Prompts to Agents**.

- `part1/`: Getting started with AI
- `part2/`: Prompt Engineering
- `part3/`: Vector Databases and RAG
- `part4/`: Fine-tuning
- `part5/`: Agents

# Getting Started

1. Clone this repository:
   ```bash
   git clone https://github.com/yourusername/developers-guide-to-ai.git
   cd developers-guide-to-ai
   ```

2. Install dependencies for a specific part (example for part1):
   ```bash
   cd part1/getting_started/section1
   npm install
   ```

3. Follow the instructions in each part's README.md for specific setup and execution steps.

# Parts Overview

## Part 1: Getting started with AI
This part provides the foundation for working with AI, featuring both JavaScript and Python implementations.

| Project | Description |
| --- | --- |
| [part1/client/](part1/client/) | A React-based frontend application that consumes the LLM API. It demonstrates how to handle streaming responses from the server and manage UI states, providing a simple interface to ask questions and see the LLM's response in real-time. |
| [part1/getting_started/](part1/getting_started/) | Contains two Express.js server implementations. `section1` provides a basic REST service that interfaces with a local LLM using LangChain. `section2` enhances this by implementing real-time streaming of LLM responses, showing how to create a more responsive user experience. |
| [part1/getting_started_python/](part1/getting_started_python/) | A FastAPI-based Python alternative to the JavaScript server. It showcases Python's asynchronous capabilities for LLM interactions, including streaming support, and demonstrates how to integrate with the same LLM backend using Python's ecosystem. |

---

## Part 2: Prompt Engineering
This part dives deeper into LLM capabilities with practical examples of prompt engineering techniques.

| Project | Description |
| --- | --- |
| [part2/basic_examples/](part2/basic_examples/) | A collection of Python scripts demonstrating fundamental prompt engineering concepts. It includes examples of temperature settings, tokenization, prompt templates, and guard rails to control LLM behavior. |
| [part2/basic_examples_javascript/](part2/basic_examples_javascript/) | JavaScript examples covering chunking, creating embeddings, and tokenization. These scripts show how to perform common NLP tasks in a JavaScript environment. |
| [part2/first_call_chain/](part2/first_call_chain/) | A simple FastAPI server that demonstrates how to create a basic LLM chain using LangChain. It shows how to combine a prompt template with an LLM to answer questions. |
| [part2/structured_output/](part2/structured_output/) | Python scripts that illustrate how to get structured data (JSON) from an LLM. It includes examples of sentiment analysis and data extraction using both the Ollama SDK and the `instructor` library. |

---

## Part 3: Vector Databases and RAG
This part focuses on building advanced AI applications using Retrieval-Augmented Generation (RAG) and vector databases.

| Project | Description |
| --- | --- |
| [part3/advanced_rag/](part3/advanced_rag/) | An advanced RAG implementation that uses a combination of BM25 and vector search (ensemble retriever) along with multi-query retrieval to enhance search results. It also includes a reranker to improve the relevance of the retrieved documents. |
| [part3/basic_examples/](part3/basic_examples/) | A Python script for experimenting with different text chunking strategies using LangChain's `RecursiveCharacterTextSplitter`. |
| [part3/client/](part3/client/) | A React-based chat interface that interacts with a RAG API. It supports conversational history and displays streaming responses from the backend. |
| [part3/conversational_rag_example/](part3/conversational_rag_example/) | A FastAPI server that implements a conversational RAG system. It uses a vector store to retrieve relevant documents and maintains conversation history to provide context-aware responses. |
| [part3/parent_document_retriever_rag/](part3/parent_document_retriever_rag/) | A RAG implementation that uses the `ParentDocumentRetriever` to handle large documents. This approach splits documents into smaller chunks for embedding and retrieval, but returns the larger parent document for better context. |
| [part3/rag_examples/](part3/rag_examples/) | A set of Python scripts that demonstrate different ways to build a RAG-based chatbot. It includes examples using both LangChain and the Ollama SDK. |
| [part3/simple_rag_javascript/](part3/simple_rag_javascript/) | A Node.js server that implements a simple RAG system in JavaScript. It uses an in-memory vector store and the `ParentDocumentRetriever` to answer questions based on a set of articles. |
| [part3/vector_databases/](part3/vector_databases/) | A collection of Python scripts that showcase various use cases for vector databases, including document search, product recommendation, and interactive product search with filtering. |

---

## Part 4: Fine-tuning
This part provides a comprehensive guide to fine-tuning models for specific tasks. It covers the entire lifecycle, from data preparation to model evaluation.

| Project | Description |
| --- | --- |
| [part4/01-dataset.ipynb](part4/01-dataset.ipynb) | A Jupyter notebook that demonstrates how to load raw data, split it into training, validation, and test sets, and save it as a Hugging Face dataset. |
| [part4/02.zeroShot.ipynb](part4/02.zeroShot.ipynb) | This notebook shows how to perform zero-shot classification using both a DistilBERT model and a Qwen model. It evaluates the accuracy of each model on a test dataset. |
| [part4/03-finetune-classificationModel.ipynb](part4/03-finetune-classificationModel.ipynb) | A notebook that fine-tunes a DistilBERT model for sequence classification. It covers tokenization, training, and evaluation of the fine-tuned model. |
| [part4/04-chat-examples.ipynb](part4/04-chat-examples.ipynb) | This notebook provides examples of how to interact with a chat model (Qwen) using different prompting strategies. |
| [part4/05-finetune-dataset.ipynb](part4/05-finetune-dataset.ipynb) | A notebook that prepares a dataset for fine-tuning a large language model (LLM). It transforms the data into a chat format suitable for training. |
| [part4/06-finetune-llm.ipynb](part4/06-finetune-llm.ipynb) | This notebook demonstrates how to fine-tune a Qwen LLM using LoRA (Low-Rank Adaptation). It sets up the trainer and runs the fine-tuning process. |
| [part4/07-test-ft-llm.ipynb](part4/07-test-ft-llm.ipynb) | A notebook for evaluating the fine-tuned LLM. It loads the model and tests its performance on the test set. |
| [part4/data/](part4/data/) | Contains the datasets used for fine-tuning, including the raw data and the processed datasets in Hugging Face format. |
| [part4/rawData/](part4/rawData/) | Contains the raw CSV files used as the source for the datasets. |

---

## Part 5: Agents
This part explores the concept of AI agents, which can use tools to interact with their environment and solve complex tasks.

| Project | Description |
| --- | --- |
| [part5/01-helloAgents.py](part5/01-helloAgents.py) | A Python script that introduces the `smol-agents` library. It demonstrates how to create a simple agent with tools like web search and browsing to answer a question. |
| [part5/02.findingEarningsReport.py](part5/02.findingEarningsReport.py) | This script showcases a more advanced agent that acts as a financial analyst. It uses tools to find and compare the quarterly revenues of different companies. |
| [part5/mcp/](part5/mcp/) | A project that demonstrates the Multi-Agent Collaboration Protocol (MCP). It includes an Express.js server with some API endpoints and an MCP server that exposes these endpoints as tools for agents to use. It also contains an example of an expense tracker agent that interacts with a MongoDB database. |

